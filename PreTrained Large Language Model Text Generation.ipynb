{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN0JKe6Mb2wL"
      },
      "source": [
        "## Packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ay2FqRIjYY2",
        "outputId": "7197b721-73e4-4b54-bc9f-8414185e71ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYYim0xGb2wN"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "import torch\n",
        "import textwrap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSSAnbyVb2wP"
      },
      "source": [
        "## Helpers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LYMeYTib2wR"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## helper function (nicer printing)\n",
        "##################################################\n",
        "\n",
        "def pretty_print(s):\n",
        "    print(\"Output:\\n\" + 80 * '-')\n",
        "    print(textwrap.fill(tokenizer.decode(s, skip_special_tokens=True),80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1a6XJcb2wR"
      },
      "source": [
        "## Obtaining a pretrained LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPnyF08wb2wS",
        "outputId": "c5500235-6659-48fb-df7b-d1dacfe78e48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model:  gpt2-large\n",
            "GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-large\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"output_scores\": true,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# model_to_use = \"gpt2\"\n",
        "model_to_use = \"gpt2-large\"\n",
        "\n",
        "print(\"Using model: \", model_to_use)\n",
        "\n",
        "# get the tokenizer for the pre-trained LM you would like to use\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n",
        "\n",
        "# instantiate a model (causal LM)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_to_use,\n",
        "                                        output_scores=True,\n",
        "                                        pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CoThfhdb2wU"
      },
      "source": [
        "## Using the LLM for text generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRN2ZRZ_b2wU",
        "outputId": "26ce31ef-bd9f-4a00-ac4d-4ef7b962d9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\ntensor([[ 7454,   257, 23952,  3214,   287,  1842,   351,   257,   279, 39291,\n           523,   326,   484]])\n\nTop-k sampling:\n\nOutput:\n--------------------------------------------------------------------------------\nOnce a vampire fell in love with a pixie so that they could continue to breed,\ntheir children were affected by the blood.  The blood turned the pixies into\nhuman beings in the process and they became responsible for killing other\nvampires, humans and creatures created by Satan himself.  They were killed in\nthe battle in 1082, as they attempted to feed on a witch named Anna.  Other\nNames  German: Aigars von Fraunhilde (literally, \"Aguaries of Fraunhilde\") â€” The\nwitch\n#+end_example"
        }
      ],
      "source": [
        "# text to expand\n",
        "prompt = \"Once a vampire fell in love with a pixie so that they\"\n",
        "\n",
        "# translate the prompt into tokens\n",
        "input_tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "print(input_tokens)\n",
        "\n",
        "outputs = model.generate(input_tokens,\n",
        "                         max_new_tokens=100,\n",
        "                         do_sample=True,\n",
        "                         top_k=50,\n",
        "                       )\n",
        "\n",
        "print(\"\\nTop-k sampling:\\n\")\n",
        "pretty_print(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0Hs1iVsb2wV",
        "outputId": "ca2c1688-864a-4835-aea4-12886dc7ea3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n\nBeam search:\n\nOutput:\n--------------------------------------------------------------------------------\nOnce a vampire fell in love with a pixie so that they could feed on her blood,\nthe pixie would become a vampire herself, and the vampire would become a pixie\nherself, and so on and so forth. The pixie would then become a vampire again,\nand then a pixie again, and so forth and so on, until the pixie became a vampire\nand the vampire became a pixie, and then the pixie was a vampire again and the\nvampire was a pixie and so on.  The pixie would eventually become a\n#+end_example"
        }
      ],
      "source": [
        "outputs = model.generate(input_tokens,\n",
        "                         max_new_tokens=100,\n",
        "                         num_beams=6,\n",
        "                         no_repeat_ngram_size=4,\n",
        "                         early_stopping=True\n",
        "                         )\n",
        "\n",
        "print(\"\\nBeam search:\\n\")\n",
        "pretty_print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W3NXf1cb2wW"
      },
      "source": [
        "## Accessing next-word probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKhgi9lDb2wW",
        "outputId": "83c79458-befa-4bb9-c2c5-8c2ad7f60273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "NLL of second word:  3.040785789489746\nNLL of whole output: 51.008323669433594"
        }
      ],
      "source": [
        "labels        = torch.clone(input_tokens)\n",
        "labels[0,0]   = -100\n",
        "output_word2  = model(input_tokens[:,0:2], labels= labels[:,0:2])\n",
        "output_prompt = model(input_tokens, labels=input_tokens)\n",
        "\n",
        "# negative log-likelihood of provided labels\n",
        "nll_word2  = output_word2.loss\n",
        "nll_output = output_prompt.loss * input_tokens.size(1)\n",
        "print(\"NLL of second word: \", nll_word2.item())\n",
        "print(\"NLL of whole output:\", nll_output.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCehU3Hpb2wX",
        "outputId": "4ac9cbfa-82ac-4d34-c559-b6681c9323b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([[[ 2.3684,  0.9006, -4.1059,  ..., -6.9914, -4.4546,  0.0598],\n         [-0.9339,  0.0542, -3.9052,  ..., -6.6439, -4.8402, -1.2681]]],\n       grad_fn=<UnsafeViewBackward0>)\ntensor([[[-0.0361, -0.3569, -0.7985,  ..., -0.8819, -0.5188, -0.2351],\n         [-3.3384, -1.2034, -0.5978,  ..., -0.5344, -0.9044, -1.5630]]],\n       grad_fn=<LogSoftmaxBackward0>)"
        }
      ],
      "source": [
        "# logits of provided labels\n",
        "print(output_word2.logits)\n",
        "# next-word log probabilities:\n",
        "print(torch.nn.functional.log_softmax(output_word2.logits, dim = 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjWqNCAmb2wY"
      },
      "source": [
        "## Accessing the embeddings (hidden states)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECMa3O4Zb2wY",
        "outputId": "cda5ef78-4be0-45cc-9055-9879a359c610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "torch.Size([1, 13, 1280])\nEmbedding of last word in input:\n tensor([ 0.0360,  0.0201, -0.0314,  ...,  0.0598,  0.0014, -0.0129],\n       grad_fn=<SelectBackward0>)"
        }
      ],
      "source": [
        "# set flag 'output_hidden_states' to true\n",
        "output = model(input_tokens, output_hidden_states = True)\n",
        "\n",
        "# this is a tuple with first element the embeddings of each token in the input\n",
        "hidden_states = output.hidden_states\n",
        "# so, access the first object from the tuple\n",
        "embeddings = hidden_states[0]\n",
        "# and print its size and content\n",
        "print(embeddings.size())\n",
        "print(\"Embedding of last word in input:\\n\", embeddings[0,0-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eatW1b8b2wZ"
      },
      "source": [
        "## [Excursion:] Using data from &rsquo;datasets&rsquo;\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z96Y-cBob2wZ",
        "outputId": "7ae4ec15-bdc7-4f89-cd8b-06d77c36678f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "--------------------------------------------------------------------------------\n",
            "  Robert Boulter is an English film, television and theatre actor. He had a\n",
            "guest @-@ starring role on the television series The Bill in 2000. This was\n",
            "followed by a starring role\n",
            "Average NLL for wikipedia chunk 3.621708869934082\n",
            "Perplexity: 37.401427417447366\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
        "\n",
        "input_tokens = encodings.input_ids[:,10:50]\n",
        "\n",
        "pretty_print(input_tokens[0])\n",
        "\n",
        "output = model(input_tokens, labels = input_tokens)\n",
        "print(\"Average NLL for wikipedia chunk\", output.loss.item())\n",
        "\n",
        "# Calculate the average cross-entropy loss\n",
        "average_loss = output.loss.item()\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = math.exp(output.loss.item())\n",
        "\n",
        "# Display perplexity\n",
        "print(\"Perplexity:\", perplexity)"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}